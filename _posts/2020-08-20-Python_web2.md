---
title: "Python: 웹크롤링2(음원차트사이트)"
last_modified_at: 2020-08-20T20:20:02-05:00
categories:
  - Python
tags:
  - Python
toc: true 
toc_label: "Table of Contents"
toc_icon: "cog"
toc_sticky: true 
author_profile: true 
read_time: false 
---

```python
#0820-1.py

# from bs4 import BeautifulSoup
# import requests
# import csv
######################################################################멜론 차트 csv 파일로 만들기(1차원)
# url = "http://www.melon.com/chart/day/index.htm"
#print(requests.get(url,headers = {"User-Agent":"Mozillar"}))  
# response = requests.get(url,headers = {"User-Agent":"Mozillar"})
# text = response.text

# html = BeautifulSoup(text, 'html.parser')

# tr_list = html.find("tbody").find_all("tr") 

# rows = []
# values = ["Rank","Title","Singer"]
# file = open("chart1.csv", "w", newline="")
# csvfile = csv.writer(file)
# csvfile.writerow(values)

# for tr in tr_list: 
    # info = []
    # td_list = tr.find_all("td")
    # info.append(tr.find("span", {"class":"rank"}).text.strip())
    # info.append(tr.find("div", {"class":"ellipsis rank01"}).text.strip())
    # info.append(tr.find("div", {"class":"ellipsis rank02"}).find("a").text.strip())
    # print(info)
    # csvfile.writerow(info)
    # rows.append(info)
#file.close()
########################################################################멜론 차트 csv 파일로 만들기(2차원)
# url = "http://www.melon.com/chart/day/index.htm"
#print(requests.get(url,headers = {"User-Agent":"Mozillar"}))  
# response = requests.get(url,headers = {"User-Agent":"Mozillar"})
# text = response.text

# html = BeautifulSoup(text, 'html.parser')

# tr_list = html.find("tbody").find_all("tr") 

# rows = []
# values = ["순위","제목","가수","앨범명"]
# file = open("chart.csv", "w", newline="", encoding = "utf-8-sig") #-sig 를 붙여서 excel 파일에서 utf-8 로 인코딩 되었다는걸 알려줘야함
# csvfile = csv.writer(file)
# csvfile.writerow(values)

# for tr in tr_list: 
    # info = []
    # td_list = tr.find_all("td")
    # info.append(tr.find("span", {"class":"rank"}).text.strip())
    # info.append(tr.find("div", {"class":"ellipsis rank01"}).text.strip())
    # info.append(tr.find("div", {"class":"ellipsis rank02"}).find("a").text.strip())
#    info.append(tr.find("div", {"class":"ellipsis rank03"}).find("a").text.strip().replace("\xed","i")) #encoding 문제 발생시 일일이 치환 하는방법
    # info.append(tr.find("div", {"class":"ellipsis rank03"}).find("a").text.strip())
    # rows.append(info)
# print(rows)
# csvfile.writerows(rows)
# file.close()
###################################################이미지파일을 다뤄보자
from bs4 import BeautifulSoup
import requests
import csv


def make_html(url):
    response = requests.get(url,headers = {"User-Agent":"Mozillar"})
    text = response.text
    html = BeautifulSoup(text, 'html.parser')
    return html


def extract_info(tr):
    rank = tr.find("span", {"class":"rank"}).text.strip()
    title = tr.find("div", {"class":"ellipsis rank01"}).text.strip()
    artist = tr.find("div", {"class":"ellipsis rank02"}).find("a").text.strip()
    album = tr.find("div", {"class":"ellipsis rank03"}).find("a").text.strip()
    return [rank, title, artist, album]

def save_csv(filename,rows):
    file = open(filename, "w", newline="", encoding = "utf-8-sig") #-sig 를 붙여서 excel 파일에서 utf-8 로 인코딩 되었다는걸 알려줘야함
    csvfile = csv.writer(file)
    csvfile.writerows(rows)
    file.close()

def save_image(tr, rank):
    img_url = tr.find("img").get("src")#get으로 속성을 볼 수 있다.
    img_url = img_url[:img_url.index("/melon")]
    response = requests.get(img_url)
    content = response.content
    filename = "melon_{:>03}.jpg".format(rank)
    file = open(filename, "wb")
    file.write(content)
    file.close()

######################################################이미지 뽑아오기
# url = "https://movie-phinf.pstatic.net/20190925_227/1569375054428gqY23_JPEG/movie_image.jpg"
# response = requests.get(url)
# content = response.content
# file = open("file1.jpeg", "wb") #바이너리 형태로 저장
# file.write(content)
# file.close()
#########################################################
# url = "http://www.melon.com/chart/day/index.htm"
# html = make_html(url)
# rows = [["순위","제목","가수","앨범명"]]
# tr_list = html.find("tbody").find_all("tr")

# for tr in tr_list[:10]:
    # row = extract_info(tr)    
    # save_image(tr, row[0])
    # rows.append(row)
# save_csv("melone_chart.csv",rows)

###########################################이제 벅스에 가서 하자
def extract_info_bugs(tr):
    rank = tr.find("div", {"class":"ranking"}).text.strip().split("\n")[0]
    title = tr.find("p", {"class":"title"}).text.strip()
    artist = tr.find("p", {"class":"artist"}).text.strip()
    album = tr.find("a", {"class":"album"}).get("title")
    return [rank, title, artist, album]
    
def save_image_bug(tr, rank):
    img_url = tr.find("img").get("src")#get으로 속성을 볼 수 있다.
    img_url = img_url[:img_url.index("?version")]
    response = requests.get(img_url)
    content = response.content
    filename = "bugs_{:>03}.jpg".format(rank)
    file = open(filename, "wb")
    file.write(content)
    file.close()

############################################################
# url = "https://music.bugs.co.kr/chart/track/day/total"
# html = make_html(url)
# rows = [["순위","제목","가수","앨범명"]]
# tr_list = html.find("tbody").find_all("tr")

# for tr in tr_list:
    # row = extract_info_bugs(tr)
    # print(row)
    # save_image_bug(tr, row[0])
    # rows.append(row)
# save_csv("bugs_chart.csv",rows)
############################################################


#20200810 끄으으읕!

date = input("날짜를 입력해주세요(yyyymmdd): ")
url = "https://music.bugs.co.kr/chart/track/day/total?chartdate={}".format(date)
html = make_html(url)
rows = [["순위","제목","가수","앨범명"]]
tr_list = html.find("tbody").find_all("tr")


for tr in tr_list:
    row = extract_info_bugs(tr)
    #print(row)
    save_image_bug(tr, row[0])
    rows.append(row)
save_csv("bugs_chart{}.csv".format(date),rows)
```